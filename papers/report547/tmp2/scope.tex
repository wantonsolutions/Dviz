%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scope}
\label{sec:scope}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our initial distributed trace time curve visualization suffered a number of
shortcomings. Mainly the time to compute the time curve using t-SNE was too
slow, on the order of 8-10 minutes for the traces we wished to analyze. Such a
compute time is problematic for us because we wished to achieve interactive
recomputation of the time curve by biasing clusters towards variables that
users marked as important. The leading cause of our visualizations compute time
was the single threaded JavaScript architecture we used to run t-SNE. Another
shortcoming of our visualization was the lack of labels on clusters, and the
size of invariants generated for a single trace. T-SNE generates visual
clustering, but not logical ones, so no further computation can be done to
the clusters. Daikon, our tool for inferring distributed invariants, outputs
all of its template invariants which are not violated during an execution. This
number can be large, and many invariants are spurious. In the following section
we describe our architectural shift from JavaScript to Go, the implementation
of an efficient  parallel t-SNE, our back end support for skewing t-SNE
clustering towards important variables. Further, we describe our algorithm for
computing visual t-SNE cluster into logical ones, our approach for calculating
cluster invariants, and their subset of unique cluster invariants.

\begin{figure}[t]
\centering
    \includegraphics[width=.5\textwidth]{fig/DvizArchetecture.pdf}

    \caption{Alterations in Dviz architecture. A) Original
    architecture with single-threaded JavaScript computation. B)
    JavaScript front end with highly paralleled Go server managing
    compute requests.}

    \label{fig:dviz-archetecture}
\end{figure}

\subsection{JavaScript to Go}
\label{sec:js2go}

JavaScript has many convenient frameworks for building client server
applications which are compatible with nearly all browsers. This fact lead us
to develop our time curve prototype in a mixture of React~\cite{react}, and
node.js~\cite{nodejs}. Calculating XOR distance on our target traces was
sufficiently slow with this framework that we cached results and served
precomputed distance from our node.js server. Figure~\ref{fig:dviz-archetecture} outlines our original architecture. T-SNE coordinates
were calculated client side, at latencies of ~30s. To achieve our goal of
interactivity we designed a new architecture with a thin client, which issues
computation requests to an optimized server written in Go. Our choice of Go was
due to its concurrency language primitives and increased performance. In
section ~\ref{sec:imp} we discuss the details of our implementation.

\subsection{Parallel t-SNE}
\label{sec:ptsne}
t-SNE is a ML algorithm for dimensionality reduction. We leverage t-SNE as the
state space of a trace point can be modelled as a high dimensional vector where
each variable is a dimension with a magnitude equal to it's binary encoding. At
it's core each iteration of t-SNE has 4 steps. First a cost gradient is
calculated, using a distance function, second the gradient is normalized, third
the gradient moves all projected points a distance, last the momentum of each
point is updated for the next step, and the points are re-projected. Each stage
of this algorithm has a data dependency on its prior step, therefore there is
no trivial method for parallelizing t-SNE. Our solution requires that separate
threads are delegated points for which they must compute values, and a master
thread which coordinates barriers to protected against inconsistent memory
accesses.



\noindent\textbf{XOR Distance:} A single point in a distributed trace can
consist of hundreds of variables.  Computing the distance between any
two points requires that we calculate XOR on each variable pair, and
then calculate the euclidean norm of each. Our typical traces consist
of ~100-300 variables per trace, and 100 trace points. As t-SNE is
$O(n^2)$ per iteration, requiring approximately 20 iterations for
reasonable results we end up with a typical number of XOR computations
in the order of ((300*300)/2)*100 * 20 ~100M XOR computations. We
alleviate much of this complexity by precomputing XOR distance for each
pairs of points and caching them. By doing so we only incur the full
cost of running XOR distance for a single iteration of t-SNE.



\begin{figure}[t]
\centering
    \includegraphics[width=.5\textwidth]{fig/put-get-curve.png}

    \caption{
        Time curve generated by our prototype running puts and gets to
        a key value store. Highlighted in green are two points which
        are both executing puts. However, their state distance is
        large, with no justification.
    }

    \label{fig:desparate-points}
\end{figure}

\subsection{variable weighting and distance reporting}
Figure~\ref{fig:desparate-points} is an example
visualization generated by our original tool. At a glance the image is
composed of 2 clusters, with no semantic meaning behind them.
In fact this is a distributed key value store responding to a 50\%
put, and 50\% get workload. Under the assumption that users would
generate their own traces from test cases, we assume that they will
have some understanding of the high level functional behaviour of
their system. To help developers reason precisely about clusters we
aimed to answer the question \emph{Why is point A distant from point
B?}.

\subsection{Cluster Detection, and Invariants}
The results of running t-SNE on high dimensional data are visual
clusters of points. While these points are useful for discerning
patterns in a trace, but as they are visual, they are not available
for further analysis without an additional processing. We propose the
use of k-means clustering on t-SNE output, to logically cluster visual
clusters. Alternative density clustering techniques such as
DBSCAN~\cite{Ester96adensity-based} have greater precision at detecting
clusters automatically, our choice of k-means is to allow users to
select the number of clusters they observe.

Logical clusters detected by k-means, can be further processed in
isolation from other clusters. Our prototype tool output distributed
data invariants which held over an entire execution. These invariants
are course grain, and do not expose invariant behaviour of individual
clusters.  
%
Daikon outputs all template invariants which are not violated by a
trace, and which are supported by entries in a trace up to a minimal
confidence measure. Applying daikon to individual clusters has the
downside that the number of detected invariants grows, as there is
less evidence to invalidate spurious invariants. We propose that
invariants unique to clusters identify their most interesting
behaviour. To detect unique invariants we compared each cluster with
all others using Daikon's \emph{Invariant Checker} tool. Unique
invariants for a cluster, are invariants which are violated by all
other clusters.



